{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning algorithms\n",
    "\n",
    "## Some general definitions\n",
    "\n",
    "- __Labels__ are the outputs when testing, or making predictions.\n",
    "\n",
    "\n",
    "## Naive Byes\n",
    "\n",
    "- It is a supervised learning algorithm. A supervised learning algorithms is the one in which the prediction is made once the model has been trained with some examples. \n",
    "\n",
    "- These examples which are used for training the model is known as dataset.\n",
    "\n",
    "- It is called ```Naive``` because it makes the assumption that the occurance of certain features is independent of the occurance of the other features.\n",
    "\n",
    "- It is based on the ```Bayes law``` which states that the probablity of ```B``` given ```A``` is equals to the probablity of the event ```A``` given ```B``` multiplied by the probability of ```A``` upon probability of ```B```.\n",
    "\n",
    "![](naive_bayes/Naive.png)\n",
    "![](naive_bayes/terminology.png)\n",
    "\n",
    "- Used for text classification, spam filtration, sentimation analysis and classifying news articles.\n",
    "\n",
    "![](naive_bayes/Baye's_theoram.png)\n",
    "\n",
    "- There are a number of naive bayes algorithm available out of which we implemented the ```GaussianNB``` to find the orignal writer of the emails with the help of the data sets given. The library used for this is ```sklearn```. \n",
    "\n",
    "- ```sklearn``` is a machine learning library which is provides a number of __supervised__ and __unsupervised__ learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of theory. Now is time for some code implementation :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create some training data\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Create a Gaussian classifier \n",
    "clf = GaussianNB()\n",
    "# Train the model with the data\n",
    "clf.fit(X,Y)\n",
    "print(clf.predict([[-0.8, -1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```partial_fit``` method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning.\n",
    "\n",
    "This is especially useful when the whole dataset is __too big__ to fit in memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# A partial fit classifier\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Support vector machine, also know as SVM's are the discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.\n",
    "\n",
    "In simple terms, they are the classifiers which find the best-fit margin in the dataset. Margin is the best fit line which makes sure that the different points are as far as possible so as to avoid the noise of the dataset.\n",
    "\n",
    "OR \n",
    "\n",
    "\n",
    "In data classification problems, SVM can be used to it provides the maximum separating margin for a linearly separable dataset. That is, of all possible decision boundaries that could be chosen to separate the dataset for classification, it chooses the decision boundary which is the most distant from the points nearest to the said decision boundary from both classes.\n",
    "\n",
    "![](SVM/0_DO1oOt94TAhfoHf6.png)\n",
    "\n",
    "There are two classes here, red and blue. The line in the middle is the decision boundary. The highlighted points are the support vectors. The distance between all of these points and the line is the maximum possible among all possible decision boundaries, thus making this the most optimal one.\n",
    "\n",
    "SVM consists of many other classifiers like [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) and [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
    "\n",
    "SVM's are used for __classification__, __regression__ and __outliers detection__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "\n",
    "# Call the SVM classifier in the same way as that of Naive Bayes \n",
    "\n",
    "# Use the SVC classifier\n",
    "clf = svm.SVC()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM's have a higher rate of accuracy than Naive Bayes. From the udacity course, we saw that the algorithm ran and gave an accuracy of ```88.4%``` in Naive Bayes Algorithm while an accuracy of ```~92%``` in the SVM linear algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear SVM vs complicated descision boundary](SVM/Linear_vs_curved.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SVM/Non-linear_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important__\n",
    "\n",
    "__Question)__ The very first point that comes into the mind is that if SVM's are there to seperate using linear hyperplanes, how come is it able to draw a circular boundary instead fo just failing?\n",
    "\n",
    "__Answer__) The equation here which we are assuming is ``` x^2+y^2 ``` which is a circle. Let us assume that the given equation is equal to ```z```. Now, lets plot the points on a 3D graph. We can observe that the blue dots here(point z) are farther than that of ```x``` or ```y``` initially.\n",
    "\n",
    "If all the ```z``` points are farther than ```x``` and ```y``` then, it is very much possible to draw a linear hyperplane (in 3D) which would cover the circular area.\n",
    "\n",
    "![](SVM/circular-data-funda.png)\n",
    "\n",
    "\n",
    "__Ques )__ Which of the three features if added would give out a linear hyperplane?\n",
    "\n",
    "![](SVM/Ques_1.png)\n",
    "\n",
    "__Ans__ |x|\n",
    "\n",
    "![](SVM/Ans_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty obvious to think that this method of adding new features is very hard. Adding a single feature would require us to make a ton of calculations which in real world applications is not feasible. A simple app that you would like to intergrate could have 10 or even 100 features and, adding these many features using this method is just simply not possible. \n",
    "\n",
    "To save us from this, there is something known as the __Kernal Trick__ \n",
    "\n",
    "![](SVM/0_DO1oOt94TAhfoHf6.png)\n",
    "\n",
    "There are two classes here, red and blue. The line in the middle is the decision boundary. The highlighted points are the support vectors. The distance between all of these points and the line is the maximum possible among all possible decision boundaries, thus making this the most optimal one.\n",
    "\n",
    "Isn’t that good now? It is, except for the fact that this is applicable; in it’s raw form; only for linearly separable data. What if it isn’t? Enter kernels. The idea is that our data, which isn’t linearly separable in our ’n’ dimensional space may be linearly separable in a higher dimensional space. To understand how kernels work, some math would be necessary so brace yourselves!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
